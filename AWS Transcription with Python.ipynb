{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VIVEKRAJ23/My-files/blob/master/AWS%20Transcription%20with%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yCkArUZaQ-r",
        "outputId": "c71a3c3f-6877-4e28-a39e-14355299c785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install boto3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.18.19-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.19\n",
            "  Downloading botocore-1.21.19-py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 64.6 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.19->boto3) (2.8.1)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.19->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.19 botocore-1.21.19 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.26.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foWadIp_aZBr"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import boto3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymVZrBZfavw4"
      },
      "source": [
        "## **Initialize the job**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPb-Yfm1aZkx"
      },
      "source": [
        "transcribe = boto3.client('transcribe',\n",
        "                          aws_access_key_id = #insert your access key ID here,\n",
        "                          aws_secret_access_key = # insert your secret access key here\n",
        "                          region_name = # region: usually, I put \"us-east-2\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1lEal1paZn4"
      },
      "source": [
        "def check_job_name(job_name):\n",
        "  job_verification = True\n",
        "  \n",
        "  # all the transcriptions\n",
        "  existed_jobs = transcribe.list_transcription_jobs()\n",
        "  \n",
        "  for job in existed_jobs['TranscriptionJobSummaries']:\n",
        "    if job_name == job['TranscriptionJobName']:\n",
        "      job_verification = False\n",
        "      break\n",
        "\n",
        "  if job_verification == False:\n",
        "    command = input(job_name + \" has existed. \\nDo you want to override the existed job (Y/N): \")\n",
        "    if command.lower() == \"y\" or command.lower() == \"yes\":\n",
        "      transcribe.delete_transcription_job(TranscriptionJobName=job_name)\n",
        "    elif command.lower() == \"n\" or command.lower() == \"no\":\n",
        "      job_name = input(\"Insert new job name? \")\n",
        "      check_job_name(job_name)\n",
        "    else: \n",
        "      print(\"Input can only be (Y/N)\")\n",
        "      command = input(job_name + \" has existed. \\nDo you want to override the existed job (Y/N): \")\n",
        "  return job_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXjsr9nEbKCn"
      },
      "source": [
        "### **For Single Speaker files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8RhKr6Ja_x5"
      },
      "source": [
        "def amazon_transcribe(audio_file_name):\n",
        "  job_uri = # your S3 access link\n",
        "  # Usually, I put like this to automate the process with the file name\n",
        "  # \"s3://bucket_name\" + audio_file_name \n",
        "\n",
        "  # Usually, file names have spaces and have the file extension like .mp3\n",
        "  # we take only a file name and delete all the space to name the job\n",
        "  job_name = (audio_file_name.split('.')[0]).replace(\" \", \"\")\n",
        "  \n",
        "  # file format\n",
        "  file_format = audio_file_name.split('.')[1]\n",
        "\n",
        "  # check if name is taken or not\n",
        "  job_name = check_job_name(job_name)\n",
        "  transcribe.start_transcription_job(\n",
        "      TranscriptionJobName=job_name,\n",
        "      Media={'MediaFileUri': job_uri},\n",
        "      MediaFormat = file_format,\n",
        "      LanguageCode='en-US')\n",
        "  \n",
        "  while True:\n",
        "    result = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
        "    if result['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
        "      print(\"FAILED\")\n",
        "      break\n",
        "    time.sleep(15)\n",
        "\n",
        "  if result['TranscriptionJob']['TranscriptionJobStatus'] == \"COMPLETED\":\n",
        "    data = pd.read_json(result['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
        "  \n",
        "  return data['results'][1][0]['transcript']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPI8DkhSbYT0"
      },
      "source": [
        "### **For Multiple Speaker files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDYMUeNgbbl-"
      },
      "source": [
        "def amazon_transcribe(audio_file_name, max_speakers = -1):\n",
        "\n",
        "  if max_speakers > 10:\n",
        "    raise ValueError(\"Maximum detected speakers is 10.\")\n",
        "\n",
        "  job_uri = \"s3 bucket link\" + audio_file_name \n",
        "  job_name = (audio_file_name.split('.')[0]).replace(\" \", \"\")\n",
        "  \n",
        "  # check if name is taken or not\n",
        "  job_name = check_job_name(job_name)\n",
        "  \n",
        "  if max_speakers != -1:\n",
        "    transcribe.start_transcription_job(\n",
        "        TranscriptionJobName=job_name,\n",
        "        Media={'MediaFileUri': job_uri},\n",
        "        MediaFormat=audio_file_name.split('.')[1],\n",
        "        LanguageCode='en-US',\n",
        "        Settings = {'ShowSpeakerLabels': True,\n",
        "                  'MaxSpeakerLabels': max_speakers\n",
        "                  }\n",
        "    )\n",
        "  else: \n",
        "    transcribe.start_transcription_job(\n",
        "        TranscriptionJobName=job_name,\n",
        "        Media={'MediaFileUri': job_uri},\n",
        "        MediaFormat=audio_file_name.split('.')[1],\n",
        "        LanguageCode='en-US',\n",
        "        Settings = {'ShowSpeakerLabels': True\n",
        "                  }\n",
        "    )    \n",
        "  \n",
        "  while True:\n",
        "    result = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
        "    if result['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
        "        break\n",
        "    time.sleep(15)\n",
        "  if result['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':\n",
        "    data = pd.read_json(result['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEbhe1_dblN5"
      },
      "source": [
        "## **Read the json files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brv2pdJlbrgU"
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "sys.path.append(\"/content/drive/My Drive/Colab Notebooks/AWS Transcribe reader\")\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/AWS Transcribe reader\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFSNdio-b0ar"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY5WZv3rb0mH"
      },
      "source": [
        "\n",
        "import json\n",
        "import datetime\n",
        "import time as ptime\n",
        "\n",
        "def read_output(filename):\n",
        "  # example filename: audio.json\n",
        "  \n",
        "  # take the input as the filename\n",
        "  \n",
        "  filename = (filename).split('.')[0]\n",
        "\n",
        "  # Create an output txt file\n",
        "  print(filename+'.txt')\n",
        "  with open(filename+'.txt','w') as w:\n",
        "    with open(filename+'.json') as f:\n",
        "\n",
        "      data=json.loads(f.read())\n",
        "      labels = data['results']['speaker_labels']['segments']\n",
        "      speaker_start_times={}\n",
        "      \n",
        "      for label in labels:\n",
        "        for item in label['items']:\n",
        "          speaker_start_times[item['start_time']] = item['speaker_label']\n",
        "\n",
        "      items = data['results']['items']\n",
        "      lines = []\n",
        "      line = ''\n",
        "      time = 0\n",
        "      speaker = 'null'\n",
        "      i = 0\n",
        "\n",
        "      # loop through all elements\n",
        "      for item in items:\n",
        "        i = i+1\n",
        "        content = item['alternatives'][0]['content']\n",
        "\n",
        "        # if it's starting time\n",
        "        if item.get('start_time'):\n",
        "          current_speaker = speaker_start_times[item['start_time']]\n",
        "        \n",
        "        # in AWS output, there are types as punctuation\n",
        "        elif item['type'] == 'punctuation':\n",
        "          line = line + content\n",
        "\n",
        "        # handle different speaker\n",
        "        if current_speaker != speaker:\n",
        "          if speaker:\n",
        "            lines.append({'speaker':speaker, 'line':line, 'time':time})\n",
        "          line = content\n",
        "          speaker = current_speaker\n",
        "          time = item['start_time']\n",
        "          \n",
        "        elif item['type'] != 'punctuation':\n",
        "          line = line + ' ' + content\n",
        "      lines.append({'speaker': speaker, 'line': line,'time': time})\n",
        "\n",
        "      # sort the results by the time\n",
        "      sorted_lines = sorted(lines,key=lambda k: float(k['time']))\n",
        "\n",
        "      # write into the .txt file\n",
        "      for line_data in sorted_lines:\n",
        "        line = '[' + str(datetime.timedelta(seconds=int(round(float(line_data['time']))))) + '] ' + line_data.get('speaker') + ': ' + line_data.get('line')\n",
        "        w.write(line + '\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yDymgX6b-ph"
      },
      "source": [
        "## **Upload files to S3 storage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIfn8_jjcJHn"
      },
      "source": [
        "# define AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and bucket_name\n",
        "# bucket_name: name of s3 storage folder\n",
        "s3 = boto3.client('s3', \n",
        "  aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
        "  aws_secret_access_key = AWS_SECRET_ACCESS_KEY,\n",
        "  region_name = \"us-east-2\")\n",
        "s3.upload_file(file_name, bucket_name, file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jb7qKIRcR0p"
      },
      "source": [
        "## **Add custom vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pKF-GiGcOlA"
      },
      "source": [
        "def vocab_name(custom_name):\n",
        "  vocab = pd.DataFrame([['Los-Angeles', np.nan, np.nan, \"Los Angeles\"], [\"F.B.I.\", \"ɛ f b i aɪ\", np.nan, \"FBI\"], [\"Etienne\", np.nan, \"eh-tee-en\", np.nan]], columns=['Phrase', 'IPA', 'SoundsLike', 'DisplayAs'])\n",
        "  vocab.to_csv(custom_name+'.csv', header=True, index=None, sep='\\t')\n",
        "  import csv\n",
        "  import time\n",
        "  csv_file = 'custom_name+'.csv\n",
        "  txt_file = 'custom_name+'.txt\n",
        "  with open(txt_file, \"w\") as my_output_file:\n",
        "    with open(csv_file, \"r\") as my_input_file:\n",
        "      my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
        "    my_output_file.close()\n",
        "  ptime.sleep(30) # wait for the file to finish\n",
        "  bucket_name = #name of the S3 bucket\n",
        "  s3.upload_file(txt_file, bucket_name, txt_file)\n",
        "  ptime.sleep(60)\n",
        "  response = transcribe.create_vocabulary(\n",
        "    VocabularyName = custom_name,\n",
        "    LanguageCode='en-US',\n",
        "    VocabularyFileUri = \"your s3 link\" + txt_file)\n",
        "    # the link usually is bucketname.region.amazonaws.com\n",
        "# after running vocab_name, we can check the status through this line\n",
        "# if it's ready, the VocabularyState will be 'READY'\n",
        "transcribe.list_vocabularies()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN4FSihCcOnP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSf9_kTFcO_1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}